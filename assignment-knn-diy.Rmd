---
title: "Assigment - kNN DIY"
author:
  - name author here - TICE97
  - name reviewer here - JuniorB2
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
   html_notebook:
    toc: true
    toc_depth: 2
---


```{r}
library(tidyverse)
library(googlesheets4)
library(class)
library(caret)
library(lattice)
library(purrr)
library(class)
library(e1071)
```

---

Choose a suitable dataset from [this](https://github.com/HAN-M3DM-Data-Mining/assignments/tree/master/datasets) folder and train  your own kNN model. Follow all the steps from the CRISP-DM model.


## Business Understanding
text and code here

## Data Understanding
text and code here

## Data Preparation
text and code here

## Modeling
text and code here

## Evaluation and Deployment
text and code here

reviewer adds suggestions for improving the model

/
## The assesment


```{r}
library(tidyverse)
library(googlesheets4)
library(class)
library(caret)
```

---
  
  Choose a suitable dataset from [this](https://github.com/HAN-M3DM-Data-Mining/assignments/tree/master/datasets) folder and train  your own kNN model. Follow all the steps from the CRISP-DM model.

## Business Understanding
Chosen dataset: Occupancy
The aim of this model is the reducing of energy waste.


## Data Understanding
There are 6 usable variables and 8143 rows
Occupancy is our target variable, the other 5 will be used to train our model
The date will be left out since it is not relevant for this assignment.
```{r}

url <- "https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/KNN-occupancy.csv"
rawDF <- read_csv(url)
str(rawDF)

```
The date column will be removed and stored in a new output called CleanDF. Also, it will be excluded to the 6th column.
```{r}

cleanDF = rawDF[-1]
summary(cleanDF[-6])

```
We can see that the ranges are very different, therefore they should be normalized in data prep. Also, we will check the occurrences of the target variable. We saw in the column above that occupancy can be a '0' or '1'. This needs to be labeled to 0=Not occupied and 1=Occupied
```{r}
cleandDF <- cleanDF %>% 
  mutate(Occupancy = factor(ifelse(cleanDF$Occupancy == 0, "not occupied", "occupied")))

summary(cleanDF)
```
```{r}
colSums(is.na(cleanDF))
```
```{r}
count_occupancy = table(cleanDF$Occupancy) # for the counts in absolute terms
print('The observations of the labels where 0 is <not occupied> and 1 is <occupied> are:' )
```
```{r}
print(count_occupancy)
```
```{r}
propOccupancy = round(prop.table(count_occupancy) * 100 , digits = 2) # to see the proportion of not occupied rooms in percentage terms

cat("\n",
    toString(propOccupancy[1]),
    "%",
    sep='')
```
```{r}
cat("\n",#to see what the room occupancy percentage is
    toString(propOccupancy[2]),
    "%",
    sep='')
```

## Data Preparation
```{r} 
#lets give them labels
cleanDF$Occupancy = factor(cleanDF$Occupancy, levels = c(0, 1), labels = c("occupied", "not occupied")) %>% relevel("not occupied")
head(cleanDF)
```


```{r}
#Now we need to normalize the data to make all the variables equally weighted

normalize = function(x) { # Function takes in a vector
  return ((x - min(x)) / (max(x) - min(x))) # distance of item value - minimum vector value divided by the range of all vector values
}
```

```{r}
#Next function will be applied to all the variables
cleanDF_n = sapply(cleanDF[-6], normalize) %>% as.data.frame()

summary(cleanDF_n)

#they are now all between ranges from 0 to 1
```
Now we will make a training set and a test set
```{r}
trainDF_feat = cleanDF_n[1:7000,]
testDF_feat = cleanDF_n[7001:8143,]

trainDF_labels = cleanDF[1:7000, 6]
testDF_labels = cleanDF[7001:8143, 6]
```

## Modeling
Here we will create a KNN model
```{r}
cleanDF_test_pred = knn(train = as.matrix(trainDF_feat), test = as.matrix(testDF_feat), cl = as.matrix(trainDF_labels), k = 6)
head(cleanDF_test_pred, 10)
```
## Evaluation
```{r}
confusionMatrix(cleanDF_test_pred, testDF_labels[[1]], positive = "occupied", dnn = c("Prediction", "Actual"))
```



## reviewer adds suggestions for improving the model
When i first saw the code i could't find the mistake, all the code was working perfectly.
Even the kNN model was working, what i did notice was that the k was set to 1.
this means that the model was only using 1 neighbor to determine what that object should be, i first thought that this was the mistake.
Later i found that the mistake was that the numbers were switched for occupancy, so occupied was labeled = 0 and not occupied was labeled = 1
You can find the mistake in row 118 to 121.
This is the code with the mistake
```{r} 
#lets give them labels
cleanDF$Occupancy = factor(cleanDF$Occupancy, levels = c(0, 1), labels = c("occupied", "not occupied")) %>% relevel("not occupied")
head(cleanDF)
```

You can rewrite the code like this:
```{r}
cleanDF$Occupancy = factor(cleanDF$Occupancy, levels = c(0, 1), labels = c("not occupied", "occupied"))
```
and then it should work.

1. How would you assess the overall performance of the model
my recommendation is that the K should be adjusted to a higher number somewhere like 5 to 8
with k = 1 you get an accuracy of 94% which is pretty good but not very helpfull.
with k = 6 you get an accuracy of 98% which is better and more reliable.

When I test it with k = 6 and run the confusion matrix this is what i get:
Confusion Matrix and Statistics

              Actual
Prediction     not occupied occupied
  not occupied          859        3
  occupied               16      265
                                        
               Accuracy : 0.9834 

the confusion matrix is tested with 1143 values, and we see the following:
859 values are labeled as 'not occupied' and all these values are True positives TP
16 values are labeled as 'not occupied' but the algorithm miscalculated that, this is what we call a False negative

256 values are labeled as 'occupied' all these values are True negatives
3 values are labeled as 'occupied' yet were not due to the miscalculation of the algorithm.

As you can see the model only miscalculated 19 values out of 1143, that is why the accuracy is so high,
overall i would say that this is a very good model, yet there is one thing that came to mind, the p-value is extremely low
it is 2.2e-16 which means that the overall probability is so low it is almost 0. Now i don't know if this is relevant but from what i have
learned in maths i would say it was important to say in my recommendation

2. What would you consider as more costly: high false negatives or high false positives levels? Why?
Well that would depend on the variables you are using, lets take the prediction model that is about people who have a heart disease or not

              Actual
Prediction     Has heart    does not have
              disease       heart disease
has heart        164            32
disease

does not have     29            251
heart disease

you can summarise the following:
164 people have a heart disease
251 people do not have a heart disease

the algorithm misclassified 29 people by saying they did not have a heart diseade when they actually did              False Negatives
the algorithm also misclassified 32 people by saying that they had a heart disease when they actualy didnt't          False Positives

with this example i would say that the people with False Negatives are more costly because they don't get 
treatment while they should and so could end up dead. 









